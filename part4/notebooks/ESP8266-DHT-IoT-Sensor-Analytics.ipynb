{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm you have installed the latest version of PixieDust on your system, run this cell\n",
    "!pip install --ignore-installed --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list --format=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataFrameFromCloudant(database):\n",
    "    \n",
    "    cloudantdata=spark.read.load(database, \"org.apache.bahir.cloudant\")\n",
    "    \n",
    "    cloudantdata.createOrReplaceTempView(\"historicaldata1\")\n",
    "    spark.sql(\"SELECT * from historicaldata1\").show()\n",
    "    return cloudantdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials from your cloudantNoSQLDB service\n",
    "hostname = \"<your cloudant hostname credential here -bluemix.cloudant.com>\"\n",
    "user = \"<your cloudant user credential here -bluemix>\"\n",
    "pw = \"<Your cloudant password credential here>\"\n",
    "\n",
    "database = \"historicaldata1\" #as long as you didn't change this in the NodeRED flow the database name stays the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .appName(\"Cloudant Spark SQL Example in Python using temp tables\")\\\n",
    "    .config(\"cloudant.host\",hostname)\\\n",
    "    .config(\"cloudant.username\", user) \\\n",
    "    .config(\"cloudant.password\", pw) \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "cloudantdata=readDataFrameFromCloudant(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "lineChart",
      "keyFields": "time",
      "timeseries": "true",
      "valueFields": "temp"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(cloudantdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"select avg(temp), std(temp), max(temp), min(temp) from historicaldata1\")\n",
    "df.show()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and show the mean and standard deviation\n",
    "df = spark.sql(\"select mean(temp) as mean, std(temp) as std from (SELECT ROW_NUMBER() OVER (ORDER BY time desc) AS row, * FROM historicaldata1 where temp is not null) where row < 30 \")\n",
    "df.show()\n",
    "\n",
    "# Calculate the zscores\n",
    "# A z-score is the number of standard deviations from the mean a data point is. \n",
    "# But more technically it's a measure of how many standard deviations below or above the population mean a raw score is.\n",
    "# A z-score is also known as a standard score and it can be placed on a normal distribution curve.\n",
    "mean = df.first().mean\n",
    "std = df.first().std\n",
    "spark.sql(\"SELECT * from (select abs( ({0}-temp)/({1}+0.0001) ) as zscore, temp,ROW_NUMBER() OVER (ORDER BY time desc) AS row FROM historicaldata1 where temp is not null order by time desc) where row < 20\".format(mean,std)).show()\n",
    "\n",
    "# Calculate the Average z-scores\n",
    "df = spark.sql(\"SELECT mean(zscore) as avgzscore from (select abs( ({0}-temp)/({1}+0.0001) ) as zscore, temp,ROW_NUMBER() OVER (ORDER BY time desc) AS row FROM historicaldata1 where temp is not null order by time desc) where row < 20\".format(mean,std))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = df.first().avgzscore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
